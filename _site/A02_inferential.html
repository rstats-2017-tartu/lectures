<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Reproducible data analysis using R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="B01_intro_to_R.html">Introduction to R</a>
</li>
<li>
  <a href="B02_graphics.html">Graphics</a>
</li>
<li>
  <a href="B03_EDA.html">Exploratory data analysis</a>
</li>
<li>
  <a href="B04_regular_expressions.html">Regular expressions</a>
</li>
<li>
  <a href="B05_tidy_data.html">Tidy data</a>
</li>
<li>
  <a href="B06_factors.html">Factors</a>
</li>
<li>
  <a href="B07_bootstrapping.html">Bootstrapping</a>
</li>
<li>
  <a href="B08_bayes_intro.html">Introduction to Bayesian data analysis</a>
</li>
<li>
  <a href="B09_bayes_continuous_vars.html">Bayes continuous variables</a>
</li>
<li>
  <a href="Bayes_tests.html">Bayesian statistical tests</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore"><ol start="4" style="list-style-type: decimal">
<li>J<c3><a4>reldav statistika</li>
</ol></h1>

</div>


<pre class="r"><code>library(tidyverse)
library(bayesboot)
library(boot)
library(rethinking) #HPDI() and PI(). Requires Stan.
#source(&quot;R/&quot;)</code></pre>
<p>Järeldav statistika püüab, kasutades keerulisi algoritme, teha valimi põhjal järeldusi statistilise populatsiooni kohta, millest see valim pärineb. Sellisel tegevusel on mõtet siis, kui ühest küljest valim peegeldab populatsiooni ja teisest küljest valim ei ole sama asi, mis populatsioon. Kui valim = populatsioon, siis pole mõtet keerulisi mudeleid kasutada, et valimi põhjal populatsiooni omadusi ennustada — seda saab teha lihtsalt valimit kirjeldades. Seega on järeldava statistika abil tehtud järeldused alati rohkem või vähem ebamäärased ning meil on vaja meetodit selle ebamäärasuse mõõtmiseks. Aga kõigepealt illustreerime valimi ja populatsiooni erinevust.</p>
<div id="valim-ei-ole-sama-mis-populatsioon" class="section level3">
<h3>4.1. Valim ei ole sama, mis populatsioon</h3>
<p>Simuleerimine on lahe sest simulatsioonid elavad mudeli väikses maailmas ning seega teame me täpselt, mida me teeme ja mida on meil selle tagajärjel oodata. Simulatsioonidega saame me hõlpsalt kontrollida, kas ja kuidas meie mudelid töötavad ning genereerida olukordi (parameetrite väärtuste kombinatsioone), mida suures maailmas kunagi ette ei tule. Selles mõttes on mudelid korraga nii väiksemad kui suuremad kui päris maailm.</p>
<p>Alustuseks simuleerime juhuvalimi n=3 lõpmata suurest normaaljaotusega populatsioonist, mille keskmine on 100 ja sd on 20. Päris elus on korraliku juhuvalimi tõmbamine tehniliselt raske ettevõtmine ja, mis veelgi olulisem, me ei tea kunagi, milline on populatsiooni tõeline jaotus, keskmine ja sd. Elagu simulatsioon!</p>
<pre class="r"><code>set.seed(1) #makes random number generation reproducible
(Sample &lt;- rnorm(n = 3, mean = 100, sd = 20)) #extra parentheses work as print()</code></pre>
<pre><code>## [1]  87.47092 103.67287  83.28743</code></pre>
<pre class="r"><code>mean(Sample)</code></pre>
<pre><code>## [1] 91.47707</code></pre>
<pre class="r"><code>sd(Sample)</code></pre>
<pre><code>## [1] 10.76701</code></pre>
<p>Nagu näha on meie valimi keskmine 10% väiksem kui peaks ja valimi sd lausa kaks korda väiksem kui peaks. Seega peegeldab meie valim halvasti populatsiooni — aga me teame seda ainult tänu sellele, et tegu on simulatsiooniga.</p>
<p>Kui juba simuleerida, siis robinal: tõmbame ühe valimi asemel 10 000, arvutame seejärel 10 000 keskmist ja sd-d ning vaatame omakorda nende statistikute jaotusi ja keskväärtusi. Simulatsioon on nagu tselluliit — see on nii odav, et igaüks võib seda endale lubada.</p>
<p>Meie lootus on, et kui meil on palju valimeid, millel kõigil on juhuslik viga, mis neid populatsiooni suhtes ühele või teisele poole kallutab, siis rohkem on valimeid, mis asuvad tõelisele populatsioonile pigem lähemal kui kaugemal.</p>
<pre class="r"><code>N &lt;- 3
N_simulations &lt;- 10000
df &lt;- tibble(a = rnorm(N * N_simulations, 100, 20), b = rep(1:N_simulations, each = N) )
Summary &lt;-  df %&gt;% group_by(b) %&gt;% summarise(Mean=mean(a), SD= sd(a) ) 
Summary %&gt;% ggplot(aes(Mean) ) + geom_histogram()</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>mean(Summary$Mean); mean(Summary$SD)</code></pre>
<pre><code>## [1] 99.98043</code></pre>
<pre><code>## [1] 17.76452</code></pre>
<p>Oh-hooo. Paljude valimite keskmiste keskmine ennustab väga täpselt populatsiooni keskmist aga sd-de keskmise keskmine alahindab populatsiooni sd-d. Valem, millega sd-d arvutatakse tõõtab lihtsalt kallutatult, kui n on väike (&lt;10). Kui ei usu, korda eelnevat simulatsiooni valimiga, mille N=30.</p>
<p>Ja nüüd 10 000 SD keskväärtused:</p>
<pre class="r"><code>Summary %&gt;% ggplot(aes(SD)) + geom_histogram()</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>mode &lt;-  function(x, adjust=1) {
  x &lt;- na.omit(x)
  dx &lt;- density(x, adjust=adjust)
y_max &lt;- dx$x[which.max(dx$y)] 
y_max
}
mode(Summary$SD)</code></pre>
<pre><code>## [1] 14.07554</code></pre>
<p>SD-de jaotus on ebasümmeetriline ja mood ehk kõige tõenäolisem valimi sd väärtus, mida võiksime oodata, on u 14, samal ajal kui populatsiooni sd = 20. Lisaks on sd-de jaotusel paks saba, mis tagab, et tesest küljest pole ka vähetõenäoline, et meie valimi sd populatsiooni sd-d kõvasti üle hindab.</p>
<p>Arvutame, mitu % valimite sd-e keskmistest on &gt; 25</p>
<pre class="r"><code>sum(Summary$SD&gt;25)/10000</code></pre>
<pre><code>## [1] 0.2114</code></pre>
<p>Me saame &gt;20% tõenäosusega pahasti ülehinnatud SD.</p>
<pre class="r"><code>sum(Summary$SD&lt;15)/10000</code></pre>
<pre><code>## [1] 0.4344</code></pre>
<p>Ja me saame &gt;40% tõenäosusega pahasti alahinnatud sd. Selline on väikeste valimite traagika. (Jooksuta sama simulatsiooni n = 100 korral.)</p>
<p>Aga vähemalt populatsiooni keskmise saame me palju valimeid tõmmates ilusasti kätte — ka väga väikeste valimitega.</p>
<p>Kahjuks pole meil ei vahendeid ega kannatust loodusest 10 000 valimi kogumiseks. Enamasti on meil üksainus valim. Õnneks pole sellest väga hullu, sest meil on olemas analoogne meetod, mis töötab üsna hästi ka ühe valimiga. Seda kutsutakse <em>bootstrappimiseks</em> ja selle võttis esimesena kasutusele parun von Münchausen. Too jutukas parun nimelt suutis end soomülkast iseenda patsi pidi välja tõmmata, mis ongi bootstrappimise põhimõte. Statistikasse tõi selle meetodi Brad Efron 1970-ndate lõpus.</p>
</div>
<div id="bootstrappimine" class="section level3">
<h3>4.2.Bootstrappimine</h3>
<blockquote>
<p>Populatsioon on valimile sama, mis valim on bootstrappitud valimile.</p>
</blockquote>
<p>Nüüd alustame ühestainsast empiirilisest valimist ja genereerime sellest 1000 virtuaalset valimit. Selleks tõmbame me oma valimist virtuaalselt 1000 uut juhuvalimit (bootstrap valimit), igaüks neist sama suur kui algne valim. Trikk seisneb selles, et bootstrap valimite tõmbamine käib asendusega, st iga empiirilise valimi element, mis bootstrap valimisse tõmmatakse, pannakse kohe ka algsesse valimisse tagasi. Seega saab seda elementi kohe uuesti samasse bootstrap valimisse tõmmata (kui juhus nii tahab). Seega sisaldab tüüpiline bootstrap valim osasid algse valimi numbreid mitmes korduses ja teisi üldse mitte. Bootstrap valimid plotitakse samamoodi, nagu me ennist tegime oma valimitega lõpmatust populatsioonist. Ainsad erinevused on, et bootstrapis on piiratud algse andmekogu suurus, millest valimeid tõmmatakse ning, et iga bootstrapi valim on sama suur kui algne andmekogu (see viimane on pigem konventsioon, kui seadus).</p>
<p>Bootstrap empiirilisele valimile suurusega n töötab nii:</p>
<ol style="list-style-type: decimal">
<li>tõmba empiirilisest valimist k uut virtuaalset valimit, igaüks suurusega n</li>
<li>arvuta keskmine, sd või mistahes muu statistik igale bootstrapi valimile. Tee seda k korda.</li>
<li>joonista oma statistiku väärtustest histogramm või density plot</li>
<li>nende andmete põhjal saab küsida palju toreidaid küsimusi — vt allpool.</li>
</ol>
<p>Mis on USA presidentide keskmine pikkus? Meil on valim 10 presidendi pikkusega.</p>
<pre class="r"><code>heights &lt;- c(183, 192, 182, 183, 177, 185, 188, 188, 182, 185) %&gt;%   as_tibble() 
n &lt;- 10 #sample size
nr_boot_samples &lt;- 1000 #the nr of bootstrap samples
a &lt;- sample_n(heights, n * nr_boot_samples, replace=TRUE)  #create random sample with replacement
a$key &lt;- rep(1:nr_boot_samples, each = n) #create a column &quot;key&quot; that cuts the sample into slices of size n
a1 &lt;- a %&gt;% group_by(key) %&gt;% summarise(Value= mean(value)) #calculate the mean for each slice of n values
dens(a1$Value)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Alternatiivne ja aeglasem kood, mis kasutab mosaic::do().</p>
<pre class="r"><code>library(mosaic)
heights &lt;- c(183, 192, 182, 183, 177, 185, 188, 188, 182, 185) %&gt;% as_tibble() 
sample_means &lt;- mosaic::do(1000) *
    (heights %&gt;% sample_n(size = 10, replace = TRUE)) 
#võta heigths andmeraamist 10-ne valim (tagasi panekuga) ja tee seda 4000 korda järjest. tulemuseks on tidy tibble, kus on veerg .index, mille järgi saab grupeerida.

sample_means1 &lt;- sample_means %&gt;% group_by(.index) %&gt;% summarise(Mean=mean(value))
    
ggplot(data = sample_means1, aes(x = Mean)) +
  geom_histogram(color = &quot;white&quot;, bins = 20)</code></pre>
<p>Nii saab do() abil 2 veeruga tibble, mille igas veerus on 3 juhuslikku arvu. NB! kuna ka tidyverse/dplyr sisaldab do() nimelist funktsiooni, on kasulik seda siin eksplitsiitselt mosaic::do() kaudu esile manada.</p>
<pre class="r"><code>mosaic::do(3)* rnorm(2)</code></pre>
<pre><code>## Using parallel package.
##   * Set seed with set.rseed().
##   * Disable this message with options(`mosaic:parallelMessage` = FALSE)</code></pre>
<pre><code>##            V1         V2
## 1  1.02486567 -2.1121608
## 2 -0.68927437  1.7594082
## 3  0.06269223 -0.8485947</code></pre>
<p>Mida selline keskväärtuste jaotus tähendab? Me võime seda vaadelda posterioorse tõenäosusjaotusena. Selle tõlgenduse kohaselt iseloomustab see jaotus täpselt meie usku presidentide keskmise pikkuse kohta, niipalju kui see usk põhineb bootstrappimises kasutatud andmetel. Senikaua, kui meil pole muid relevantseid andmeid, on kõik, mida me usume teadvat USA presidentide keskmise pikkuse kohta, peidus selles jaotuses. Need pikkused, mille kohal jaotus on kõrgem, sisaldavad meie jaoks tõenäolisemalt tegelikku USA presidentide keskmist pikkust kui need pikkused, mille kohal posterioorne jaotus on madalam.</p>
<p>Kuidas selle jaotusega edasi töötada? See on lihtne: meil on 1000 arvu ja me teeme nendega kõike seda, mida parasjagu tahame.</p>
<p>Näiteks me võime arvutada, millisesse pikkuste vahemikku jääb 92% meie usust USA presidentide tõelise keskmise pikkuse kohta. See tähendab, et teades seda vahemikku peaksime olema valmis maksma mitte rohkem kui 92 senti pileti eest, mis juhul kui USA presidentide keskmine pikkus tõesti jääb sinna vahemikku, toob meile võidu suuruses 1 EUR (ja 8 senti kasumit). Selline kihlveokontor on muide täiesti respektaabel ja akadeemiline tõenäosuse tõlgendus; see on paljude arvates lausa parim tõlgendus, mis meil on.</p>
<p>Miks just 92% usaldusinterval? Vastus on, et miks mitte? Meil pole ühtegi universaalset põhjust eelistada üht usaldusvahemiku suurust teisele. Olgu meil usaldusinteval 90%, 92% või 95% — tõlgendus on ikka sama. Nimelt, et me usume, et suure tõenäosusega jääb tegelik keskväärtus meie poolt arvutatud vahemikku. Mudeli ja maailma erinevused tingivad niikuinii selle, et konkreetne number ei kandu mudelist üle pärismaailma. NB! pane tähele, et eelnevalt mainitud kihlveokontor töötab mudeli maailmas, mitte teie kodu lähedasel hipodroomil.</p>
<p>92% usaldusintervalli arvutamiseks on kaks meetodit, mis enamasti annavad vaid veidi erinevaid tulemusi. 1) HPDI — Highest Density Probability Interval — alustab jaotuse tipust (tippudest) ja isoleerib 92% jaotuse kõrgema(te) osa(de) pindalast 2) PI — Probability Interval — alustab jaotuse servadest ja isoleerib kummagist servast 4% jaotuse pindalast. See on sama, mis arvutada 4% ja 96% kvantiilid</p>
<pre class="r"><code>HPDI(a1$Value, prob=0.92) #highest probability density interval - non-symmetric</code></pre>
<pre><code>## |0.92 0.92| 
## 182.5 186.9</code></pre>
<pre class="r"><code>PI(a1$Value, prob=0.92) #symmetric method.</code></pre>
<pre><code>##    4%   96% 
## 182.2 186.7</code></pre>
<p>HPDI on üldiselt parem mõõdik kui PI, aga teatud juhtudel on seda raskem arvutada. Kui HPDI ja PI tugevalt erinevad, on hea mõte piirduda jaotuse enda avaldamisega — jaotus ise sisaldab kogu informatsiooni, mis meil on oma statistiku väärtuse kohta. Intervallid on lihtsalt summaarsed statistikud andmete kokkuvõtlikuks esitamiseks.</p>
<p>Kui suure tõenäosusega on USA presidentide keskmine pikkus suurem kui USA populatsiooni meeste keskmine pikkus (178.3 cm mediaan)?</p>
<pre class="r"><code>sum(a1$Value&gt;178.3)/4000</code></pre>
<pre><code>## [1] 0.25</code></pre>
<pre class="r"><code>mode(a1$Value)</code></pre>
<pre><code>## [1] 184.55</code></pre>
<p>Ligikaudu 100% tõenäosusega (valimis on 1 mees alla 182 cm, ja tema on 177 cm). Lühikesed jupatsid ei saa Ameerikamaal presidendiks!</p>
<p>Edaspidi kasutame bootstrappimiseks veidi moodsamat meetodit — Bayesian bootstrap — mis töötab veidi paremini väikeste valimite korral. Aga üldiselt on tulemused sarnased.</p>
<pre class="r"><code>library(bayesboot)
# Heights of the last ten American presidents in cm (Kennedy to Obama).
heights &lt;- c(183, 192, 182, 183, 177, 185, 188, 188, 182, 185);
b1 &lt;- bayesboot(heights, mean)
plot(b1)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>#summary(b1)
#hist(b1)
HPDI(b1$V1, prob=0.95)</code></pre>
<pre><code>##    |0.95    0.95| 
## 182.3218 186.9762</code></pre>
<pre class="r"><code># it&#39;s more efficient to use the a weighted statistic (but you can use a normal statistic like mean() or median() as well - as above).
b2 &lt;- bayesboot(heights, weighted.mean, use.weights = TRUE)</code></pre>
<p>It can also be easily post processed.</p>
<pre class="r"><code># the probability that the mean is &gt; 182 cm.
mean( b1[,1] &gt; 182)</code></pre>
<pre><code>## [1] 0.984</code></pre>
<p>2 keskväärtuse erinevus (keskmine1 - keskmine2):</p>
<pre class="r"><code>df &lt;- tibble(a=rnorm(10), b=rnorm(10,1,1))
m1 &lt;- bayesboot(df$a, mean)
m2 &lt;- bayesboot(df$b, mean)
m12 &lt;- bind_cols(m1, m2) 
m12 &lt;- m12 %&gt;% mutate(value=m12[,2] - m12[,1])
hist(m12$value)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>median(m12$value)</code></pre>
<pre><code>## [1] 1.632339</code></pre>
<pre class="r"><code>mode(m12$value)</code></pre>
<pre><code>## [1] 1.624736</code></pre>
<pre class="r"><code>HPDI(m12$value)</code></pre>
<pre><code>##     |0.89     0.89| 
## 0.9859806 2.3107054</code></pre>
<pre class="r"><code>#library(BayesianFirstAid); bayes.t.test(m1, m2) 
#will give a similar, but fully Bayesian, result
#requires JAGS.</code></pre>
<p><strong>Bayesian bootstrap SD arvutamiseks.</strong></p>
<p>When use.weights = FALSE it is important that the summary statistics does not change as a function of sample size. This is the case with the sample standard deviation, so here we have to implement a function calculating the population standard deviation.</p>
<pre class="r"><code>pop.sd &lt;- function(x) {
  n &lt;- length(x)
  sd(x) * sqrt( (n - 1) / n)
}

b3 &lt;- bayesboot(heights, pop.sd)
summary(b3)</code></pre>
<p><strong>A Bayesian bootstrap analysis of a correlation coefficient</strong></p>
<pre class="r"><code>#Data comparing two methods of measuring blood flow.
blood.flow &lt;- data.frame(
  dye = c(1.15, 1.7, 1.42, 1.38, 2.80, 4.7, 4.8, 1.41, 3.9),
  efp = c(1.38, 1.72, 1.59, 1.47, 1.66, 3.45, 3.87, 1.31, 3.75))

# Using the weighted correlation (corr) from the boot package.
library(boot)
b4 &lt;- bayesboot(blood.flow, corr, R = 1000, use.weights = TRUE)
plot(b4)</code></pre>
<p><strong>A Bayesian bootstrap analysis of lm coefficients</strong></p>
<pre class="r"><code># A custom function that returns the coefficients of
# a weighted linear regression on the blood.flow data
lm.coefs &lt;- function(d, w) {
  coef( lm(efp ~ dye, data = d, weights = w) )
}

b5 &lt;- bayesboot(blood.flow, lm.coefs, R = 1000, use.weights = TRUE)
plot(b5)</code></pre>
<p>Bootstrappimine on väga hea meetod, mis sõltub väiksemast arvust eeldustest kui statistikas tavaks. Bootstrap ei eelda, et andmed on normaaljaotusega või mõne muu matemaatiliselt lihtsa jaotusega. Tema põhiline eeldus on, et valim peegeldab populatsiooni — mis ei pruugi kehtida väikeste valimite korral ja kallutatud (mitte-juhuslike) valimite korral.</p>
<div id="parameetriline-bootstrap" class="section level4">
<h4>Parameetriline bootstrap</h4>
<p>Kui me arvame, et me teame, mis jaotusega on meie andmed, ja meil on suhteliselt vähe andmepunkte, võib olla mõistlik lisada bootstrapile andmete jaotuse mudel. Näiteks, meie USA presidentide pikkused võiksid olla umbkaudu normaaljaotusega (sest me teame, et USA meeste pikkused on seda). Seega fitime kõigepealt presidentide pikkusandmetega normaaljaotuse ja seejärel tõmbame bootsrap valimid sellest normaaljaotuse mudelist. Normaaljaotuse mudelil on 2 parameetrit: keskmine (mu) ja standardhälve (sigma), mida saame fittida valimiandmete põhjal:</p>
<pre class="r"><code>mu &lt;- mean(heights)
sigma &lt;- sd(heights)
N &lt;- length(heights)
sample_means &lt;- tibble(value=rnorm(N*1000, mu, sigma), indeks=rep(1:1000, each=N)) 

sample_means1 &lt;- sample_means %&gt;% group_by(indeks) %&gt;% summarise(Mean=mean(value))
    
ggplot(data = sample_means1, aes(x = Mean)) +
  geom_histogram(color = &quot;white&quot;, bins = 20)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>HPDI(sample_means1$Mean)</code></pre>
<pre><code>##    |0.89    0.89| 
## 182.6090 186.7144</code></pre>
<p>Võrdluseks, tavaline mitteparameetriline bootstrap annab vastuseks <code>HPDI(a1$Value)</code></p>
<pre class="r"><code>HPDI(a1$Value)</code></pre>
<pre><code>## |0.89 0.89| 
## 182.6 186.6</code></pre>
<p>Bootstrappimisel (nii parameetrilisel kui mitteparameetrilisel versioonil) on üks konseptuaalne puudus, mis on eriti tõsine väikeste valimite korral. Võtame oma näite USA presidentidest. Meie valimi liikmed on kõik presideerinud viimase 50 aasta jooksul, aga selle aja jooksul on inimeste keskmine pikkus jõudsalt kasvanud. Kui me tahame ennustada järgmise 50 aasta keskmist presidentide pikkust, peaksime selle faktiga arvestama. Aga bootstrap ei jäta meile sellist võimalust. Vähemalt mitte lihtsat.</p>
<p>Siin tuleb appi Bayesi statistika oma täies hiilguses. Selles paradigmas ei arvesta me mitte ainult andmetega vaid ka taustateadmistega, sünteesides need kokku üheks posterioorseks jaotuseks ehk järeljaotuseks. Selle jaotuse arvutamine erineb bootstrapist, kuid tema tõlgendus ja praktiline töö sellega on samasugune. Erinevalt tavapärasest bootstrapist on Bayes parameetriline meetod, mis sõltub andmete modelleerimisest modeleerija poolt ette antud jaotustesse (normaaljaotus, t jaotus jne). Tegelikult peame me Bayesi arvutuseks modelleerima vähemalt kaks erinevat jaotust: andmete jaotus, mida me kutsume likelihoodiks ehk tõepäraks, ning eelneva teadmise mudel ehk prior, mida samuti modeleeritakse tõenäosusjaotusena.</p>
</div>
</div>
<div id="bayesi-pohimote" class="section level2">
<h2>4.3. Bayesi põhimõte</h2>
<p>Bayesi arvutuseks on meil vaja teada</p>
<ol style="list-style-type: decimal">
<li><p>milline on “<em>parameter space</em>” ehk parameetriruum? Parameetriruum koosneb kõikidest loogiliselt võimalikest parameetriväärtustest. Näiteks kui me viskame ühe korra münti, koosneb parameetriruum kahest elemendist: 0 ja 1, ehk null kulli ja üks kull. See ammendab võimalike sündmuste nimekirja. Kui me aga hindame mõnd pidevat suurust (keskmine pikkus, tõenäosus 0 ja 1 vahel jms), koosneb parameetriruum lõpmata paljudest elementidest (arvudest).</p></li>
<li><p>milline on “<em>likelihood function</em>” ehk tõepärafunktsioon? Me omistame igale parameetriruumi elemendile (igale võimalikule parameetri väärtusele) tõepära. Tõepära parameetri väärtusel x on tõenäosus, millega me võiksime kohata oma andmete keskväärtust, juhul kui x oleks see ainus päris õige parameetri väärtus. Teisisõnu, tõepära on kooskõla määr andmete ja parameetri väärtuse x vahel. Tõepära = Pr(andmed | parameetri väärtus). Näiteks, kui tõenäolised on meie andmed, kui USA keskmine president on juhtumisi 183.83629 cm pikkune? Kuna meil on vaja modeleerida tõepära igal võimalikul parameetri väärtusel (mida pideva suuruse puhul on lõpmatu hulk), siis kujutame tõepära pideva funktsioonina (näiteks normaaljaotusena), mis täielikult katab parameetriruumi. Tõepärafunktsioon ei summeeru 100-le protsendile — see on normaliseerimata.</p></li>
<li><p>milline on “<em>prior function</em>” ehk prior? Igale tõepära väärtusele peab vastama priori väärtus. Seega, kui tõepära on modelleeritud pideva funktsioonina, siis on ka prior pidev funktsioon (aga prior ei pea olema sama tüüpi funktsioon, kui tõepära). Erinevus tõepära ja priori vahel seisneb selles, et kui tõepärafunktsioon annab just meie andmete keskväärtuse tõenäosuse igal parameetriväärtusel, siis prior annab iga parameetriväärtuse tõenäosuse, sõltumata meie andmetest. See-eest arvestab prior kõikide teiste relevantsete andmetega, sünteesides taustateadmised ühte tõenäousmudelisse. Me omistame igale parameetriruumi väärtusele eelneva tõenäosuse, et see väärtus on üks ja ainus tõene väärtus. Prior jaotus summeerub 1-le. Prior kajastab meie konkreetsetest andmetest sõltumatut arvamust, kui suure tõenäosusega on just see parameetri väärtus tõene; seega seda, mida me usume enne oma andmete nägemist. Nendel parameetri väärtustel, kus prior (või tõepära) = 0%, on ka posteerior garanteeritult 0%. See tähendab, et kui te olete 100% kindel, et mingi sündmus on võimatu, siis ei suuda ka mäekõrgune hunnik uusi andmeid teie uskumust muuta (eelduselt, et te olete ratsionaalne inimene).</p></li>
</ol>
<p><a href="http://optics.eee.nottingham.ac.uk/match/uncertainty.php" class="uri">http://optics.eee.nottingham.ac.uk/match/uncertainty.php</a> aitab praktikas priorit modelleerida (proovige <em>Roulette</em> meetodit).</p>
<p>Kui te eelnevast päriselt aru ei saanud, ärge muretsege. Varsti tulevad puust ja punaseks näited likelihoodi ja priori kohta.</p>
<p>Edasi on lihtne. Arvuti võtab tõepärafunktsiooni ja priori, korrutab need üksteisega läbi ning seejärel normaliseerib saadud jaotuse nii, et jaotusealune pindala võrdub ühega. Saadud tõenäosusjaotus ongi posteeriorne jaotus ehk posteerior ehk järeljaotus. Kogu lugu.</p>
<p>Me teame juba pool sajandit, et Bayesi teoreem on sellisele ülesandele parim võimalik lahendus. Lihtsamad ülesanded lahendame me selle abil täiuslikult. Kuna parameetrite arvu kasvuga mudelis muutub Bayesi teoreemi läbiarvutamine eksponentsiaalselt arvutusmahukamaks (sest läbi tuleb arvutada mudeli kõikide parameetrite kõikide väärtuste kõikvõimalikud kombinatsioonid), oleme sunnitud vähegi keerulisemad ülesanded lahendama umbkaudu, asendades Bayesi teoreemi <em>ad hoc</em> MCMC algoritmiga, mis teie arvutis peituva propelleri Karlsoni kombel lendu saadab selle nimel, et tõmmata valim “otse” posterioorsest jaotusest. Meie poolt kasutatava MCMC <em>Hamiltonian Monte Carlo</em> mootori nimi on Stan. See on R-st eraldiseisev programm, millel on siiski R-i liides R-i pakettide rstan(), rethinking(), rstanarm() jt kaudu. Meie töötame ka edaspidi puhtalt R-s, mis automaatselt suunab meie mudelid ja muud andmed Stani, kus need läbi arvutatakse ja seejärel tulemused R-i tagasi saadetakse. Tulemuste töötlus ja graafiline esitus toimub jällegi R-is. Seega ei pea me ise kordagi Stani avama.</p>
<p>Alustame siiski lihtsa näitega, mida saab käsitsi läbi arvutada.</p>
<div id="naide" class="section level3">
<h3>1. näide</h3>
<p>Me teame, et suremus haigusesse on 50% ja meil on palatis 3 patsienti, kes seda haigust põevad. Seega on meil kaks andmetükki (50% ja n=3). Küsimus: mitu meie patsienti oodatavalt hinge heidavad? Eeldusel, et meie patsiendid on iseseisvad (näiteks ei ole sugulased), on meil tüüpiline mündiviske olukord.</p>
<p>Parameetriruum: 0 surnud, 1 surnud, 2 surnud ja 3 surnud. Seega on meil neljaliikmeline parameetriruumi.</p>
<p>Edasi loeme üles kõik võimalikud sündmusteahelad, mis saavad loogiliselt juhtuda (et saada tõepärafunktsioon).</p>
<p>Me viskame kulli-kirja 3 korda: H - kiri, T - kull</p>
<p>Võimalikud sündmused on:</p>
<p>HHH, HTH, THH, HHT, HTT, TTH, THT, TTT,</p>
<p>Kui Pr(H) = 0.5 ning H = elus ja T = surnud, siis lugedes kokku kõik võimalikud sündmused:</p>
<ul>
<li>0 surnud - 1,</li>
<li>1 surnud - 3,</li>
<li>2 surnud - 3,</li>
<li>3 surnud - 1</li>
</ul>
<p>Nüüd teame parameetriruumi igale liikme kohta, kui suure tõenäosusega me ootame selle realiseerumist. Näiteks, Pr(0 surnud) = 1/8, Pr(1 surnud) = 3/8, Pr(1 või 2 surnud) = 6/8 jne</p>
<p>Selle teadmise saab konverteerida tõepärafunktsiooniks</p>
<pre class="r"><code>x&lt;-seq(from= 0, to=3) #parameter space as a grid
y&lt;-c(1,3,3,1) #likelihood - 
plot(x,y, ylab=&quot;number of possibilities&quot;, xlab=&quot;number of deaths&quot;, type=&quot;b&quot;, main=&quot;likelihood&quot;)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Siit näeme, et üks surm ja kaks surma on sama tõenäolised ja üks surm on kolm korda tõenäolisem kui null surma (või kolm surma).</p>
<p>Tõepära annab meile tõenäosuse Pr(mortality=0.5 &amp; N=3) igale loogiliselt võimalikule surmade arvule (0 kuni 3).</p>
<p>Me saame sama tulemuse kasutades binoomjaotuse mudelit (tegelikult kasutasime ka ennist sama mudelit). Ainus erinevus on, et nüüd on meil y teljel surmade tõenäosus.</p>
<pre class="r"><code>x&lt;-seq(0, 3)
y &lt;- dbinom(x,3, 0.5)
plot(x, y, type=&quot;b&quot;, xlab=&quot;nr of deaths&quot;, ylab=&quot;probability of x deaths&quot;, main=&quot;probability of x deaths out of 3 patients if Pr(Heads)=0.5&quot;)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Nüüd proovime seda koodi olukorras, kus meil on 9 patsienti ja suremus on 0.67</p>
<pre class="r"><code>x&lt;-seq(0, 9)
y &lt;- dbinom(x,9, 0.67)
plot(x, y, type=&quot;b&quot;, xlab=&quot;nr of deaths&quot;, ylab=&quot;probability of x deaths&quot;, main=&quot;probability of x out of 9 deaths if Pr(Heads)=0.67&quot;)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Lisame sellele tõepärafunktsioonile tasase priori (lihtsuse huvides) ja arvutame posterioorse jaotuse kasutades Bayesi teoreemi. Igale parameetri väärtusele on tõepära * prior proportsionaalne posterioorse tõenäosusega, et just see parameetri väärtus on see ainus tõene väärtus. Posterioorsed tõenäosused normaliseeritakse nii, et nad summeeruksid 1-le.</p>
<p>Me defineerime X telje kui rea 10-st arvust (0 kuni 9 surma) ja arvutame tõepära igale neist 10-st arvust. Sellega ammendame me kõik loogiliselt võimalikud parameetri väärtused.</p>
<pre class="r"><code># define grid
p_grid &lt;- seq( from=0 , to=9 )
# define flat prior
prior &lt;- rep( 1 , 10 )
# compute likelihood at each value in grid
likelihood &lt;- dbinom( p_grid , size=9 , prob=0.67 )

# compute product of likelihood and prior
unstd.posterior &lt;- likelihood * prior 

# normalize the posterior, so that it sums to 1
posterior &lt;- unstd.posterior/sum(unstd.posterior)
# sum(posterior) == 1 

plot( x = p_grid , y = posterior , type=&quot;b&quot; ,
    xlab=&quot;nr of deaths&quot; , ylab=&quot;posterior probability&quot;, main=&quot;posterior distribution&quot; )</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>See on parim võimalik teadmine, mitu kirstu tasuks tellida, arvestades meie priori ja likelihoodi mudelitega. Näiteks, sedapalju, kui surmad ei ole üksteisest sõltumatud, on meie tõepäramudel (binoomjaotus) vale.</p>
</div>
<div id="naide-sonastame-oma-probleemi-umber" class="section level3">
<h3>2. näide: sõnastame oma probleemi ümber</h3>
<p>Mis siis, kui me ei tea suremust ja tahaksime seda välja arvutada? Kõik, mida me teame on, et 6 patsienti 9st surid. Nüüd koosnevad andmed 9 patsiendi morbiidsusinfost (parameeter, mille väärtust me eelmises näites arvutasime) ja parameeter, mille väärtust me ei tea, on surmade üldine sagedus (see parameeter oli eelmises näites fikseeritud, ja seega kuulus andmete hulka).</p>
<p>Seega on meil 1) parameetriruum 0% kuni 100% suremus (0st 1-ni), mis sisaldab lõpmata palju numbreid.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>kaks võimalikku sündmust (surnud, elus), seega binoomjaotusega modelleeritud tõepärafunktsioon. Nagu me juba teame, on r funktsioonis dbinom() kolm argumenti: surmade arv, patsientide koguarv ja surmade tõenäosus. Seekord oleme me fikseerinud esimesed kaks ja soovime arvutada kolmanda väärtuse.</p></li>
<li><p>tasane prior, mis ulatub 0 ja 1 vahel. Me valisime selle priori selleks, et mitte muuta tõepärafunktsiooni kuju. See ei tähenda, et me arvaksime, et tasane prior on mitteinformatiivne. Tasane prior tähendab, et me usume, et suremuse kõik väärtused 0 ja 1 vahel on võrdselt tõenäolised. See on vägagi informatsioonirohke (ebatavaline) viis maailma näha, ükskõik mis haiguse puhul!</p></li>
</ol>
<p><strong>Tõepära parameetri väärtusel x on tõenäosus kohata meie andmeid, kui x on juhtumisi parameetri tegelik väärtus.</strong> Meie näites koosneb tõepärafunktsioon tõenäosustest, et kuus üheksast patsiendist surid igal võimalikul suremuse väärtusel (0…1). Kuna see on lõpmatu rida, teeme natuke sohki ja arvutame tõepära 20-l valitud suremuse väärtusel</p>
<pre><code>Tehniliselt on sinu andmete tõepärafunktsioon agregeeritud iga üksiku 
andmepunkti tõepärafunktsioonist. Seega vaatab Bayes igat andmepunkti 
eraldi (andmete sisestamise järjekord ei loe).</code></pre>
<pre class="r"><code># define grid (mortality at 20 evenly spaced probabilities from 0 to 1)
p_grid &lt;- seq( from=0 , to=1 , length.out=20 )
# define prior
prior &lt;- rep( 1 , 20 )
# compute likelihood at each value in grid
likelihood &lt;- dbinom( 6 , size=9 , prob=p_grid )
#plot prior
plot(p_grid, prior, type=&quot;b&quot;, main=&quot;prior&quot;)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre class="r"><code>#plot likelihood
plot(p_grid, likelihood, type=&quot;b&quot;, main=&quot;the likelihood function&quot;)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-27-2.png" width="672" /></p>
<pre class="r"><code># compute product of likelihood and prior &amp; standardize the posterior.
posterior &lt;- likelihood * prior / sum(likelihood * prior)
#plot posterior
plot( x = p_grid , y = posterior , type=&quot;b&quot; ,
    xlab=&quot;mortality&quot; , ylab=&quot;posterior probability&quot;, main=&quot;posterior distribution&quot; )</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-27-3.png" width="672" /></p>
<p>Nüüd on meil posterioorne tõenäosusfunktsioon, mis summeerub 1-le ja mis sisaldab kogu meie teadmist suremuse kohta.</p>
<blockquote>
<p>Alati on kasulik plottida kõik kolm funktsiooni (tõepära, prior ja posteerior).</p>
</blockquote>
<div id="kui-n1" class="section level4">
<h4>Kui n=1</h4>
<p>Bayes on lahe sest tema hinnangud väiksele N-le on loogiliselt sama pädevad kui suurele N-le. See ei ole nii klassikalises sageduslikus statistikas, kus paljud testid on välja töötatud N = Inf eeldusel ja töötavad halvasti väikeste valimitega.</p>
<p>Hea küll, me arvutame jälle suremust.</p>
<p>Bayes töötab andmepunkti kaupa (see et me talle ennist kõik andmed korraga ette andsime, on puhtalt meie mugavus)</p>
<pre class="r"><code># define grid
p_grid &lt;- seq( from=0 , to=1 , length.out=20 )
# define prior
prior &lt;- rep( 1 , 20 )
# compute likelihood at each value in grid
likelihood &lt;- dbinom( 1 , size=1 , prob=p_grid )
posterior &lt;- likelihood * prior / sum(likelihood * prior)

plot( x = p_grid , y = posterior , type=&quot;b&quot; ,
    xlab=&quot;mortality&quot; , ylab=&quot;posterior probability&quot; )</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>esimene patsient suri - 0 mortaalsus ei ole enam loogiliselt võimalik (välja arvatud siis kui prior selle koha peal = 0) ja mortaalsus 100% on andmetega (tegelikult andmega) parimini kooskõlas. Posteerior on nulli ja 100% vahel sirge sest vähene sissepandud informatsioon lihtsalt ei võimalda enamat.</p>
<pre class="r"><code># define grid
p_grid &lt;- seq( from=0 , to=1 , length.out=20 )
# define prior
prior &lt;- posterior
# compute likelihood at each value in grid
likelihood &lt;- dbinom( 1 , size=1 , prob=p_grid )
posterior1 &lt;- likelihood * prior / sum(likelihood * prior)

plot( x = p_grid , y = posterior1 , type=&quot;b&quot; ,
    xlab=&quot;mortality&quot; , ylab=&quot;posterior probability&quot; )</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Teine patsient suri. Nüüd ei ole 0 ja 1 vahel enam sirge posteerior. Posteerior on kaldu 100 protsendi poole, mis on ikka kõige tõenäolisem väärtus.</p>
<pre class="r"><code># define grid
p_grid &lt;- seq( from=0 , to=1 , length.out=20 )
# define prior
prior &lt;- posterior1
# compute likelihood at each value in grid
likelihood &lt;- dbinom( 0 , size=1 , prob=p_grid )
# compute product of likelihood and prior
posterior2 &lt;- likelihood * prior / sum(likelihood * prior)

plot( x = p_grid , y = posterior2 , type=&quot;b&quot; ,
    xlab=&quot;mortality&quot; , ylab=&quot;posterior probability&quot; )</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Kolmas patsient jäi ellu - 0 ja 100% mortaalsus on seega võimaluste nimekirjast maas ning suremus on ikka kaldu valimi keskmise poole (75%).</p>
<p>Teeme sedasama prioriga, mis ei ole tasane. See illustreerib tõsiasja, et kui N on väike siis domineerib prior posteerior jaotust. (Suure N korral on vastupidi, priori kuju on sageli vähetähtis.)</p>
<pre class="r"><code># define grid
p_grid &lt;- seq( from=0 , to=1 , length.out=20 )
# define prior
prior &lt;- c( seq(1:10), seq(from= 10, to= 1) )
# compute likelihood at each value in grid
likelihood &lt;- dbinom( 1 , size=1 , prob=p_grid )
posterior &lt;- likelihood * prior / sum(likelihood * prior)

plot(x=1:20, y=prior, type=&quot;b&quot;, main=&quot;prior&quot;)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<pre class="r"><code>plot( x = p_grid , y = posterior , type=&quot;b&quot; ,
    xlab=&quot;mortality&quot; , ylab=&quot;posterior probability&quot;, main=&quot;posterior&quot;)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-31-2.png" width="672" /> 1. patsient suri</p>
<pre class="r"><code># define grid
p_grid &lt;- seq( from=0 , to=1 , length.out=20 )
# define prior
prior &lt;- posterior
# compute likelihood at each value in grid
likelihood &lt;- dbinom( 1 , size=1 , prob=p_grid )
# compute product of likelihood and prior
posterior1 &lt;- likelihood * prior / sum(likelihood * prior)

plot(x=1:20, y=prior, type=&quot;b&quot;, main=&quot;prior&quot;)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<pre class="r"><code>plot( p_grid , posterior1 , type=&quot;b&quot; ,
    xlab=&quot;mortality&quot; , ylab=&quot;posterior probability&quot;, main=&quot;posterior&quot; )</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-32-2.png" width="672" /></p>
<p>Teine patsient suri</p>
<pre class="r"><code># define grid
p_grid &lt;- seq( from=0 , to=1 , length.out=20 )
# define prior
prior &lt;- posterior1
# compute likelihood at each value in grid
likelihood &lt;- dbinom( 0 , size=1 , prob=p_grid )
posterior2 &lt;- likelihood * prior / sum(likelihood * prior)

plot(x=1:20, y=prior, type=&quot;b&quot;, main=&quot;prior&quot;)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code>plot( x = p_grid , y = posterior2 , type=&quot;b&quot; ,
    xlab=&quot;mortality&quot; , ylab=&quot;posterior probability&quot;, main=&quot;posterior&quot;  )</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-33-2.png" width="672" /> 3. patsient jäi ellu. Nüüd on posteeriori tipp mitte 75% juures nagu ennist, vaid kuskil 50% juures — tänu priorile.</p>
</div>
</div>
</div>
<div id="mudelite-keel" class="section level2">
<h2>4.4. Mudelite keel</h2>
<p>Siin vaatame kuidas kirjeldada mudelit nii, et masin selle ära tunneb. Meie mudelid töötavad läbi rethinking() paketi. See raamatukogu pakub kaks võimalust, kuidas mudelit arvutada, mis mõlemad kasutavad sama notatsiooni. Mõlemad võimalused arvutavad posteeriori mitte Bayesi teoreemi kasutades (nagu me ennist tegime), vaid kasutades stohhastilisi meetodeid, mis iseloomustavad posteeriori umbkaudu (aga piisavalt täpselt). Põhjuseks on, et keerulisemate mudelite korral on Bayesi teoreemi kasutamine liialt arvutusmahukas.</p>
<p>Esiteks rethinking::map() leiab posteeriori tipu ja selle lähedal funktsiooni tõusunurga. Siin on eelduseks, et posteerior on normaaljaotus. See eeldus kehtib alati, kui nii prior kui tõepära on modelleeritud normaaljaotusena (ja ka paljudel muudel juhtudel).</p>
<p>Teine võimalus on rethinking::map2stan(), mis suunab teie kirjutatud mudeli Stan-i. Stan teeb <em>Hamilonian Monte Carlo</em> simulatsiooni, kasutades valget maagiat selleks, et tõmmata valim otse posteerioorsest jaotusest. See on väga moodne lähenemine statistikale, töötab oluliselt aeglasemalt kui map, aga ei sõltu normaaljaotustest ning suudab arvutada hierarhilisi mudeleid, mis map-le üle jõu käivad.</p>
<p>Me võime sama mudeli kirjelduse sõõta mõlemasse funktsiooni.</p>
<p>Lihtne mudel näeb välja niimodi:</p>
<p>dead ~ dbinom(9, p) , # binomial likelihood</p>
<p>p ~ dunif(0, 1) # uniform prior</p>
<p>Tõepärafunktsioon on modeleeritud binoomjaotusena. Parameeter, mille väärtust määratakse on p, ehk suremus. See on ainus parameeter, mille väärtust me siin krutime. NB! igale parameetrile peab vastama oma prior. Meil on selles mudelis täpselt 1 parameeter ja 1 prior. Vastuseks saame selle ainsa parameetri posterioorse jaotuse. Hiljem näeme, et kui meil on näiteks 452 parameetrit, mille väärtusi me koos arvutame, siis on meil ka 452 priorit ja 452 posterioorset jaotust.</p>
<pre class="r"><code>library(rethinking)
m1 &lt;- map(
    alist(
        dead ~ dbinom(9, p) ,  # binomial likelihood
        p ~ dunif(0, 1)     # uniform prior
), data=list(dead=6) )

precis( m1 ) # summary of quadratic approximation</code></pre>
<pre><code>##   Mean StdDev 5.5% 94.5%
## p 0.67   0.16 0.42  0.92</code></pre>
<p>Nüüd tõmbame posteerioroorsest jaotusest valimi n=10 000. Selleks on funktsioon extract.samples()</p>
<pre class="r"><code>samples&lt;-extract.samples(m1)
#hist(samples$p)
dens(samples$p)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<pre class="r"><code>#PI(samples$p, prob = 0.95) #leaves out equal 2.5% at both sides
HPDI(samples$p, prob = 0.95) #highest density 95% at the center</code></pre>
<pre><code>##     |0.95     0.95| 
## 0.3607887 0.9729674</code></pre>
<p>Kuus patsienti üheksast surid ja nüüd me usume, et tegelik suremus võib olla nii madal kui 37% ja nii kõrge kui 97%. Kui me tahame paremat hinnangut on meil vaja kas rohkem patsiente või informatiivsemat priorit (paremat taustainfot).</p>
<pre class="r"><code>library(rethinking)
m2 &lt;- map(
    alist(
        dead ~ dbinom(90,p) ,  # binomial likelihood
        p ~ dunif(0,1)     # uniform prior
), data=list(dead=60) )
# display summary of quadratic approximation
precis( m2 )</code></pre>
<pre><code>##   Mean StdDev 5.5% 94.5%
## p 0.67   0.05 0.59  0.75</code></pre>
<pre class="r"><code>samples&lt;-extract.samples(m2)
dens(samples$p)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<pre class="r"><code>#PI(samples$p, prob = 0.95) #leaves out equal 2.5% at both sides
HPDI(samples$p, prob = 0.95) #highest density 95% at the center</code></pre>
<pre><code>##     |0.95     0.95| 
## 0.5685874 0.7651462</code></pre>
<p>10 korda rohkem andmeid: nüüd on suremus määratud kuskile 57% ja 77% vahele (suure tõenäosusega)</p>
</div>
<div id="beta-prior" class="section level2">
<h2>4.5. beta prior</h2>
<p>Nüüd anname sisse mõistlikuma struktuuriga priori: beta-jaotuse</p>
<p>Beta-prior katab vahemiku 0st 1ni ja sellel on 2 parameetrit, a ja b.</p>
<p>Siin mõned näited erinevatest beta parametriseeringutest</p>
<pre class="r"><code>x &lt;- seq(0, 1, length = 1000)
plot(x, dbeta(x, 0.2, 0.2))</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<pre class="r"><code>plot(x, dbeta(x, 1, 0.2))</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-38-2.png" width="672" /></p>
<pre class="r"><code>plot(x, dbeta(x, 1, 1))</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-38-3.png" width="672" /></p>
<pre class="r"><code>plot(x, dbeta(x, 2, 1))</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-38-4.png" width="672" /></p>
<pre class="r"><code>plot(x, dbeta(x, 4, 1))</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-38-5.png" width="672" /></p>
<pre class="r"><code>plot(x, dbeta(x, 2, 2))</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-38-6.png" width="672" /></p>
<pre class="r"><code>plot(x, dbeta(x, 4, 4))</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-38-7.png" width="672" /></p>
<pre class="r"><code>plot(x, dbeta(x, 200, 100))</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-38-8.png" width="672" /></p>
<p>beta(θ | a, b) jaotuse keskväärtus on</p>
<p><em>μ = a/(a + b)</em></p>
<p>ja mood on</p>
<p><em>ω = (a − 1)/(a + b − 2)</em> (juhul kui <em>a &gt; 1</em> ja <em>b &gt; 1</em>).</p>
<p>Seega, kui a=b, siis on keskmine ja mood 0.5. Kui a &gt; b, on keskmine ja mood &gt; 0.5 ja kuid a &lt; b, on mõlemad &lt; 0.5.</p>
<p>Beta jaotuse “laiuse” annab “konsentratsioon” <em>κ = a + b</em>. Mida suurem κ, seda kitsam jaotus.</p>
<p><em>a = μκ</em></p>
<p><em>b = (1 − μ)κ</em></p>
<p><em>a = ω(κ − 2) + 1</em></p>
<p><em>b = (1 − ω)(κ − 2) + 1</em> for <em>κ &gt; 2</em></p>
<p>Me võime κ-le omistada väärtuse nagu see oleks mündivisete arv, mis iseloomustab meie priori tugevust (juhul kui tõepära funktsioon tuleb andmetest, mis koosnevad selle sama mündi visetest). Kui meie jaoks piisaks ainult mõnest mündiviset, et priorist (eelnevast teadmisest) lahti ütelda, peaks meie prior sisaldama väikest kappat.</p>
<pre><code>Näiteks, mu prior on, et münt on aus (μ = 0.5; a = b), aga ma ei ole selles väga kindel. 
Niisiis ma arvan, et selle eelteadmise kaal võrdub sellega, kui ma oleksin näinud 8 mündiviske tulemust. 
Seega κ = 8, mis tähendab, et a = μκ = 4 and b = (1 − μ)κ = 4. 
Aga mis siis kui me tahame beta priorit, mille mood ω = 0.8 ja κ = 12? 
Siis saame valemist, et a = 9 ja b = 3. </code></pre>
<pre class="r"><code>library(rethinking)
m3 &lt;- map(
    alist(
        dead ~ dbinom(9,p) ,  # binomial likelihood
        p ~ dbeta(200,100)     # beta prior
), data=list(dead=6) )
# display summary of quadratic approximation
precis( m3 )</code></pre>
<pre><code>##   Mean StdDev 5.5% 94.5%
## p 0.67   0.03 0.62  0.71</code></pre>
<pre class="r"><code>samples&lt;-extract.samples(m3)
dens(samples$p)</code></pre>
<p><img src="A02_inferential_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<pre class="r"><code>HPDI(samples$p, prob = 0.95) #highest density 95% at the center</code></pre>
<pre><code>##     |0.95     0.95| 
## 0.6148913 0.7203977</code></pre>
<p>Nagu näha on ka kitsa priori mõju üsna väika, isegi kui kui n=9.</p>
</div>
<div id="lopetuseks-veel-prioritest-uldiselt." class="section level2">
<h2>4.6. Lõpetuseks veel prioritest üldiselt.</h2>
<p>Neid võib jagada kolmeks: mitteinformatiivsed, väheinformatiivsed ehk “regularizing” ja informatiivsed.</p>
<p>Mitteinformatiivseid prioreid ei ole sisuliselt olemas ja neid on soovitav vältida. Sageli kutsutakse tasaseid prioreid mitteinformatiivseteks. Neil on vähemalt 2 puudust. Tasane prior, mis ulatub lõpmatusse, on tehniliselt “improper”, sest tema alune pindala ei summeeru ühele. Ja teiseks muudavad sellised priorid mcmc ahelad vähem efektiivseteks, mis võib teie arvutuse kihva keerata.</p>
<p>Väheinformatiivsed priorid kujutavad endast kompromissi: nad muudavad võimalikult vähe tõepärafunktsiooni kuju, aga samas piiravad seda osa parameetriruumist, kust MCMC ahelad posteeriori otsivad (mis on arvutuslikult soodne). Nende taga on filosoofiline eeldus, et teadlast huvitavad eelkõige tema enda andmed ja see, mida need ühe või teise hüpoteesi (parameetri väärtuse) kohta ütlevad. See eeldus on vaieldav aga kui selle järgi käia, siis kulub vähem mõttejõudu eelteadmiste mudelisse formaliseerimiseks.</p>
<p>Vähemalt suured farmaatsiafirmad seda hoiakut ei jaga ja kulutavad usinalt oma miljoneid korralike informatiivsete priorite tootmiseks. Selles protsessis saavad kokku statistikud, teaduseksperdid ja psühholoogid, et inimkonna teadmisi võimalikult adekvaatselt vormida tõenäosusjaotustesse. Meie töötame siin siiski enamasti väheinformatiivsete prioritega, mis on hetkel moes. Aga teile oma teaduses soovitan siiralt arendada informatiivseid prioreid. Vähemalt nõnda toimides te mõtlete oma teaduse üle põhjalikult järele.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
